{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8c91e26-0ebc-4ad9-86e1-e049e4bdbcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from datetime import datetime\n",
    "from typing import List, Tuple, Optional\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "226c6ec1-8f95-4df5-b88d-6e456857c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the folder to the path\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "# Now import normally\n",
    "import sam2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac0ecf48-5735-49a1-83b8-9cdf1b8bfd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2_video_predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f13297ea-9d12-4ead-8461-68f2d6cf234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM2LongProcessor:\n",
    "    SAM_DIR = \"/users/5/ribei056/software/python/sam2\"\n",
    "    SAM2LONG_DIR = \"//users/5/ribei056/software/python/SAM2Long/sam2\"\n",
    "    def __init__(self):\n",
    "        self.frame_rate_render = 6\n",
    "        self.visualization_step = 15\n",
    "        self.unique_id = None\n",
    "        self.outdir = None\n",
    "        self.start_frame = 0\n",
    "        self.inference_state = None\n",
    "        self.predictor = None\n",
    "        self.first_frame_path = None\n",
    "        self.tracking_points = []\n",
    "        self.trackings_input_label = []\n",
    "        self.video_frames_dir = None\n",
    "        self.scanned_frames = []\n",
    "        self.frame_names = []\n",
    "        self.available_frames = []\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    def load_model(self, checkpoint=\"tiny\"):\n",
    "        \"\"\"Load the SAM2 model based on the specified checkpoint size\"\"\"\n",
    "        if checkpoint == \"tiny\":\n",
    "            sam2_checkpoint = f\"{self.SAM_DIR}/checkpoints/sam2.1_hiera_tiny.pt\"\n",
    "            model_cfg = f\"{self.SAM2LONG_DIR}/configs/sam2.1/sam2.1_hiera_t.yaml\"\n",
    "        elif checkpoint == \"small\":\n",
    "            sam2_checkpoint = f\"{self.SAM_DIR}/checkpoints/sam2.1_hiera_small.pt\"\n",
    "            model_cfg = f\"{self.SAM2LONG_DIR}/configs/sam2.1/sam2.1_hiera_s.yaml\"\n",
    "        elif checkpoint == \"base-plus\":\n",
    "            sam2_checkpoint = f\"{self.SAM_DIR}/checkpoints/sam2.1_hiera_base_plus.pt\"\n",
    "            model_cfg = f\"{self.SAM2LONG_DIR}/configs/sam2.1/sam2.1_hiera_b+.yaml\"\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid checkpoint: {checkpoint}\")\n",
    "\n",
    "        print(f\"Loading checkpoint: {checkpoint}, ({sam2_checkpoint}, {model_cfg})\")\n",
    "        self.predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=self.device)\n",
    "        print(\"Model loaded successfully\")\n",
    "        return self.predictor\n",
    "\n",
    "    def preprocess_video(self, video_path, outdir, max_duration=60):\n",
    "        \"\"\"\n",
    "        Extract frames from a video file.\n",
    "        Args:\n",
    "            video_path: Path to the video file\n",
    "            outdir: Output directory path\n",
    "            max_duration: Maximum duration to process in seconds\n",
    "        Returns:\n",
    "            Path to the first frame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Generate a unique ID based on the video filename\n",
    "            self.unique_id = os.path.splitext(os.path.basename(video_path))[0]\n",
    "\n",
    "            # Create output directory\n",
    "            self.outdir = outdir\n",
    "            extracted_frames_output_dir = f'{self.outdir}/frames_{self.unique_id}'\n",
    "\n",
    "            # Create output directories if they don't exist\n",
    "            os.makedirs(extracted_frames_output_dir, exist_ok=True)\n",
    "            os.makedirs(f\"{self.outdir}/frames_output_images\", exist_ok=True)\n",
    "\n",
    "            # Check if frames already exist\n",
    "            existing_frames = [\n",
    "                p for p in os.listdir(extracted_frames_output_dir)\n",
    "                if os.path.splitext(p)[-1].lower() in [\".jpg\", \".jpeg\"]\n",
    "            ]\n",
    "\n",
    "            # If frames already exist, skip extraction\n",
    "            if existing_frames:\n",
    "                print(f\"Found {len(existing_frames)} existing frames, skipping extraction\")\n",
    "                self.scanned_frames = existing_frames\n",
    "                self.scanned_frames.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "                self.video_frames_dir = extracted_frames_output_dir\n",
    "                self.first_frame_path = os.path.join(extracted_frames_output_dir, self.scanned_frames[0])\n",
    "                print(f\"Using existing frames from: {extracted_frames_output_dir}\")\n",
    "                print(f\"First frame at: {self.first_frame_path}\")\n",
    "                return self.first_frame_path\n",
    "\n",
    "            # Open the video file\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            if not cap.isOpened():\n",
    "                raise ValueError(f\"Error: Could not open video file {video_path}\")\n",
    "\n",
    "            try:\n",
    "                # Get video properties\n",
    "                fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "                if fps <= 0:\n",
    "                    print(f\"Warning: Invalid FPS ({fps}), using default value of 30\")\n",
    "                    fps = 30\n",
    "\n",
    "                max_frames = int(fps * max_duration)\n",
    "\n",
    "                # Extract frames\n",
    "                frame_number = 0\n",
    "                while True:\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret or frame_number >= max_frames:\n",
    "                        break\n",
    "\n",
    "                    if frame_number % self.frame_rate_render == 0:  # Save every nth frame\n",
    "                        frame_filename = os.path.join(extracted_frames_output_dir, f'{frame_number:05d}.jpg')\n",
    "                        # Check if frame file already exists\n",
    "                        if not os.path.exists(frame_filename):\n",
    "                            cv2.imwrite(frame_filename, frame)\n",
    "\n",
    "                    # Store first frame path\n",
    "                    if frame_number == 0:\n",
    "                        self.first_frame_path = os.path.join(extracted_frames_output_dir, f'{frame_number:05d}.jpg')\n",
    "\n",
    "                    frame_number += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error during frame extraction: {str(e)}\")\n",
    "                raise\n",
    "            finally:\n",
    "                # Release video resources\n",
    "                cap.release()\n",
    "\n",
    "            # Scan all JPEG frames\n",
    "            self.scanned_frames = [\n",
    "                p for p in os.listdir(extracted_frames_output_dir)\n",
    "                if os.path.splitext(p)[-1].lower() in [\".jpg\", \".jpeg\"]\n",
    "            ]\n",
    "\n",
    "            if not self.scanned_frames:\n",
    "                raise ValueError(f\"No frames were extracted from the video {video_path}\")\n",
    "\n",
    "            self.scanned_frames.sort(key=lambda p: int(os.path.splitext(p)[0]))\n",
    "            self.video_frames_dir = extracted_frames_output_dir\n",
    "\n",
    "            print(f\"Processed {len(self.scanned_frames)} frames from video\")\n",
    "            print(f\"First frame saved at: {self.first_frame_path}\")\n",
    "\n",
    "            # Return the path to access the first frame\n",
    "            return self.first_frame_path\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Video file not found: {video_path}\")\n",
    "            raise\n",
    "        except PermissionError:\n",
    "            print(f\"Error: Permission denied when accessing {video_path} or {outdir}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def add_point(self, x, y, start_frame_idx, point_type=\"include\"):\n",
    "        \"\"\"\n",
    "        Add a tracking point\n",
    "        \"\"\"\n",
    "        self.tracking_points.append([x, y])\n",
    "        if start_frame_idx < 0:\n",
    "            raise ValueError(\"Start frame index must be >= 0\")\n",
    "        self.start_frame = ((start_frame_idx + self.frame_rate_render - 1) // self.frame_rate_render) * self.frame_rate_render\n",
    "        self.start_frame_idx = self.start_frame // self.frame_rate_render\n",
    "\n",
    "        # Add label (1 for include, 0 for exclude)\n",
    "        if point_type == \"include\":\n",
    "            self.trackings_input_label.append(1)\n",
    "        elif point_type == \"exclude\":\n",
    "            self.trackings_input_label.append(0)\n",
    "        else:\n",
    "            raise ValueError(\"Point type must be 'include' or 'exclude'\")\n",
    "\n",
    "        print(f\"Added {point_type} point at ({x}, {y}) starting at frame {self.start_frame}\")\n",
    "        return self.tracking_points, self.trackings_input_label\n",
    "\n",
    "    def clear_points(self):\n",
    "        \"\"\"Clear all tracking points\"\"\"\n",
    "        self.tracking_points = []\n",
    "        self.trackings_input_label = []\n",
    "        print(\"All points cleared\")\n",
    "        return self.tracking_points, self.trackings_input_label\n",
    "\n",
    "    def get_mask(self, checkpoint=\"tiny\"):\n",
    "        \"\"\"\n",
    "        Process the first frame with SAM2 to get the initial mask\n",
    "\n",
    "        Args:\n",
    "            checkpoint: Model size (\"tiny\", \"small\", \"base-plus\")\n",
    "\n",
    "        Returns:\n",
    "            Path to the output image with mask\n",
    "        \"\"\"\n",
    "        if not self.predictor:\n",
    "            self.load_model(checkpoint)\n",
    "\n",
    "        if not self.tracking_points:\n",
    "            raise ValueError(\"No tracking points added. Use add_point() first.\")\n",
    "\n",
    "        # Init inference state if not already done\n",
    "        if self.inference_state is None:\n",
    "            self.inference_state = self.predictor.init_state(video_path=self.video_frames_dir)\n",
    "            self.inference_state['num_pathway'] = 3\n",
    "            self.inference_state['iou_thre'] = 0.3\n",
    "            self.inference_state['uncertainty'] = 2\n",
    "            self.inference_state['device'] = self.device\n",
    "\n",
    "        # Process the points\n",
    "        ann_frame_idx = self.start_frame_idx # First frame\n",
    "        ann_obj_id = 1  # Object ID\n",
    "\n",
    "        points = np.array(self.tracking_points, dtype=np.float32)\n",
    "        labels = np.array(self.trackings_input_label, np.int32)\n",
    "\n",
    "        _, out_obj_ids, out_mask_logits = self.predictor.add_new_points(\n",
    "            inference_state=self.inference_state,\n",
    "            frame_idx=ann_frame_idx,\n",
    "            obj_id=ann_obj_id,\n",
    "            points=points,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.title(f\"Initial Mask (frame {ann_frame_idx})\")\n",
    "        plt.imshow(Image.open(os.path.join(self.video_frames_dir, self.scanned_frames[ann_frame_idx])))\n",
    "        self._show_points(points, labels, plt.gca())\n",
    "        self._show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0])\n",
    "\n",
    "        # Save output\n",
    "        output_filename = f\"{self.outdir}/output_first_frame.jpg\"\n",
    "        plt.savefig(output_filename, format='jpg')\n",
    "        plt.close()\n",
    "\n",
    "        self.frame_names = self.scanned_frames\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Initial mask generated and saved to {output_filename}\")\n",
    "        return output_filename\n",
    "\n",
    "    def propagate(self, vis_frame_type=\"check\", checkpoint=\"tiny\"):\n",
    "        \"\"\"\n",
    "        Propagate mask to all frames\n",
    "\n",
    "        Args:\n",
    "            vis_frame_type: \"check\" (every 15 frames) or \"render\" (all frames)\n",
    "            checkpoint: Model size\n",
    "\n",
    "        Returns:\n",
    "            If vis_frame_type is \"check\": List of output image paths\n",
    "            If vis_frame_type is \"render\": Path to the output video\n",
    "        \"\"\"\n",
    "        if not self.inference_state:\n",
    "            raise ValueError(\"No inference state. Call get_mask() first.\")\n",
    "\n",
    "        # Use bfloat16 for efficiency if possible\n",
    "        if torch.cuda.is_available():\n",
    "            torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "\n",
    "            if torch.cuda.get_device_properties(0).major >= 8:\n",
    "                torch.backends.cuda.matmul.allow_tf32 = True\n",
    "                torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "        # Make sure the right model is loaded\n",
    "        if self.predictor is None:\n",
    "            self.load_model(checkpoint)\n",
    "\n",
    "        # Ensure device is set correctly\n",
    "        if torch.cuda.is_available():\n",
    "            self.inference_state[\"device\"] = 'cuda'\n",
    "        else:\n",
    "            self.inference_state[\"device\"] = 'cpu'\n",
    "\n",
    "        # Clear output directory\n",
    "        frames_output_dir = f\"{self.outdir}/frames_output_images\"\n",
    "        self.cleanup_temp_files()\n",
    "\n",
    "        # Run propagation\n",
    "        print(f\"Starting mask propagation across {self.inference_state['num_frames']} frames...\")\n",
    "        start_time = datetime.now()\n",
    "        out_obj_ids, out_mask_logits = self.predictor.propagate_in_video(\n",
    "            self.inference_state,\n",
    "            start_frame_idx=self.start_frame_idx,\n",
    "            reverse=False\n",
    "        )\n",
    "        propagation_time = datetime.now() - start_time\n",
    "        print(f\"Mask propagation completed in {propagation_time.total_seconds():.2f} seconds\")\n",
    "\n",
    "        # Store results\n",
    "        print(\"Processing mask results...\")\n",
    "        video_segments = {}\n",
    "        for frame_idx in range(0, self.inference_state['num_frames']-self.start_frame_idx):\n",
    "            video_segments[frame_idx] = {\n",
    "                out_obj_ids[0]: (out_mask_logits[frame_idx] > 0.0).cpu().numpy()\n",
    "            }\n",
    "\n",
    "        # Determine visualization stride\n",
    "        vis_frame_stride = self.visualization_step if vis_frame_type == \"check\" else 1\n",
    "\n",
    "        # Calculate total frames to process\n",
    "        total_frames = len(range(0, len(self.frame_names), vis_frame_stride))\n",
    "        print(f\"Rendering {total_frames} frames with masks...\")\n",
    "\n",
    "        # Render frames with masks in batches\n",
    "        jpeg_images = []\n",
    "        batch_size = 100  # Adjust based on your memory constraints\n",
    "        processed_frames = 0\n",
    "\n",
    "        for start_idx in range(0, len(self.frame_names), batch_size):\n",
    "            batch_frames = []\n",
    "            end_idx = min(start_idx + batch_size, len(self.frame_names))\n",
    "            batch_start_time = datetime.now()\n",
    "\n",
    "            print(f\"Processing batch {start_idx//batch_size + 1}/{(len(self.frame_names) + batch_size - 1)//batch_size}...\")\n",
    "\n",
    "            for out_frame_idx in range(start_idx, end_idx, vis_frame_stride):\n",
    "                if out_frame_idx < self.start_frame_idx:\n",
    "                    continue\n",
    "                plt.figure(figsize=(6, 4))\n",
    "                plt.title(f\"frame {out_frame_idx}\")\n",
    "\n",
    "                # Open and process image\n",
    "                img = Image.open(os.path.join(self.video_frames_dir, self.frame_names[out_frame_idx]))\n",
    "                plt.imshow(img)\n",
    "\n",
    "                if out_frame_idx >= self.start_frame_idx:\n",
    "                    for out_obj_id, out_mask in video_segments[out_frame_idx-self.start_frame_idx].items():\n",
    "                        self._show_mask(out_mask, plt.gca(), obj_id=out_obj_id)\n",
    "\n",
    "                # Save frame\n",
    "                output_filename = os.path.join(frames_output_dir, f\"frame_{out_frame_idx}.jpg\")\n",
    "                plt.savefig(output_filename, format='jpg')\n",
    "                plt.close()\n",
    "                img.close()  # Explicitly close PIL image\n",
    "\n",
    "                batch_frames.append(output_filename)\n",
    "                processed_frames += 1\n",
    "\n",
    "                # Print progress every 10 frames or at specific percentages\n",
    "                if processed_frames % 10 == 0 or processed_frames == total_frames:\n",
    "                    progress_percent = (processed_frames / total_frames) * 100\n",
    "                    print(f\"Progress: {processed_frames}/{total_frames} frames ({progress_percent:.1f}%)\")\n",
    "\n",
    "            jpeg_images.extend(batch_frames)\n",
    "\n",
    "            batch_time = datetime.now() - batch_start_time\n",
    "            frames_per_second = len(batch_frames) / batch_time.total_seconds() if batch_time.total_seconds() > 0 else 0\n",
    "            print(f\"Batch completed in {batch_time.total_seconds():.2f} seconds ({frames_per_second:.2f} frames/second)\")\n",
    "\n",
    "            # Force garbage collection\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        total_time = datetime.now() - start_time\n",
    "        print(f\"Total processing time: {total_time.total_seconds():.2f} seconds\")\n",
    "\n",
    "        if vis_frame_type == \"check\":\n",
    "            print(f\"Generated {len(jpeg_images)} sample frames with masks\")\n",
    "            return jpeg_images\n",
    "        elif vis_frame_type == \"render\":\n",
    "            # Create video from frames\n",
    "            print(\"Creating output video...\")\n",
    "            video_start_time = datetime.now()\n",
    "\n",
    "            cap = cv2.VideoCapture(os.path.join(self.video_frames_dir, self.frame_names[0]))\n",
    "            original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "            cap.release()\n",
    "\n",
    "            # Instead of creating ImageSequenceClip with all images at once\n",
    "            # Use a more memory-efficient approach\n",
    "            output_path = f\"{self.outdir}/output_video.mp4\"\n",
    "\n",
    "            # Option 1: Create clip with lazy loading\n",
    "            # clip = ImageSequenceClip(jpeg_images, fps=original_fps // self.frame_rate_render,\n",
    "            #                          load_images=False)  # Lazy loading\n",
    "            # clip.write_videofile(output_path, codec='libx264', threads=4,\n",
    "            #          ffmpeg_params=[\"-pix_fmt\", \"yuv420p\"],  # Direct FFmpeg parameters\n",
    "            #          logger=None)\n",
    "\n",
    "            # Option 2: Use ffmpeg directly\n",
    "            # import subprocess\n",
    "            cmd = f\"ffmpeg -framerate {original_fps // self.frame_rate_render} -i {frames_output_dir}/frame_%d.jpg -c:v mjpeg {output_path}\"\n",
    "            print(f\"Running command: {cmd}\")\n",
    "            subprocess.call(cmd, shell=True)\n",
    "            video_time = datetime.now() - video_start_time\n",
    "            print(f\"Video creation completed in {video_time.total_seconds():.2f} seconds\")\n",
    "            print(f\"Generated output video: {output_path}\")\n",
    "            return output_path\n",
    "        else:\n",
    "            raise ValueError(\"Invalid vis_frame_type. Use 'check' or 'render'.\")\n",
    "\n",
    "    def _show_mask(self, mask, ax, obj_id=None, random_color=False):\n",
    "        \"\"\"Helper function to display mask on pyplot axis\"\"\"\n",
    "        if random_color:\n",
    "            color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "        else:\n",
    "            cmap = plt.get_cmap(\"tab10\")\n",
    "            cmap_idx = 0 if obj_id is None else obj_id\n",
    "            color = np.array([*cmap(cmap_idx)[:3], 0.6])\n",
    "\n",
    "        h, w = mask.shape[-2:]\n",
    "        mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "        ax.imshow(mask_image)\n",
    "\n",
    "    def _show_points(self, coords, labels, ax, marker_size=200):\n",
    "        \"\"\"Helper function to display points on pyplot axis\"\"\"\n",
    "        pos_points = coords[labels == 1]\n",
    "        neg_points = coords[labels == 0]\n",
    "\n",
    "        if len(pos_points) > 0:\n",
    "            ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*',\n",
    "                       s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "        if len(neg_points) > 0:\n",
    "            ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*',\n",
    "                       s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "\n",
    "    def cleanup_temp_files(self):\n",
    "        \"\"\"\n",
    "        Clean up temporary files created during video generation.\n",
    "        Removes all temporary frame images from frames_output_images directory.\n",
    "        \"\"\"\n",
    "        frames_output_dir = f\"{self.outdir}/frames_output_images\"\n",
    "        if os.path.exists(frames_output_dir):\n",
    "            file_count = 0\n",
    "            for f in os.listdir(frames_output_dir):\n",
    "                file_path = os.path.join(frames_output_dir, f)\n",
    "                try:\n",
    "                    if os.path.isfile(file_path):\n",
    "                        os.remove(file_path)\n",
    "                        file_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error removing file {file_path}: {e}\")\n",
    "\n",
    "            print(f\"Cleaned up {file_count} temporary files from {frames_output_dir}\")\n",
    "        else:\n",
    "            print(f\"Temporary directory {frames_output_dir} not found\")\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse command line arguments\"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"SAM2Long Video Segmenter\")\n",
    "    parser.add_argument(\"--video\", type=str, help=\"Path to input video\")\n",
    "    parser.add_argument(\"--points\", type=str, help=\"Points as 'x1,y1:1;x2,y2:0' where 1=include, 0=exclude\")\n",
    "    parser.add_argument(\"--frame\", type=int, default=0, help=\"Frame to start process\")\n",
    "    parser.add_argument(\"--checkpoint\", type=str, default=\"tiny\",\n",
    "                        choices=[\"tiny\", \"small\", \"base-plus\"],\n",
    "                        help=\"Model checkpoint to use\")\n",
    "    parser.add_argument(\"--outdir\", type=str, default=\"output\", help=\"Output directory\")\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function for command line usage\"\"\"\n",
    "    args = parse_args()\n",
    "\n",
    "    # Validate video file exists and is a valid video file\n",
    "    if not os.path.isfile(args.video):\n",
    "        raise FileNotFoundError(f\"Video file not found: {args.video}\")\n",
    "\n",
    "    # Check file extension (simple validation)\n",
    "    valid_extensions = ['.mpeg', '.mpg', '.mp4', '.mov']\n",
    "    if not any(args.video.lower().endswith(ext) for ext in valid_extensions):\n",
    "        raise ValueError(f\"Invalid video file format. Supported formats: {', '.join(valid_extensions)}\")\n",
    "\n",
    "    # Initialize processor\n",
    "    processor = SAM2LongProcessor()\n",
    "\n",
    "    # Process video\n",
    "    processor.preprocess_video(args.video, args.outdir)\n",
    "\n",
    "    # Parse and add points\n",
    "    if args.points:\n",
    "        # Validate points format using regex\n",
    "        point_pattern = r'^\\d+,\\d+:[01](?:;\\d+,\\d+:[01])*$'\n",
    "        if not re.match(point_pattern, args.points):\n",
    "            raise ValueError(\"Invalid points format. Expected format: 'x1,y1:1;x2,y2:0' where x,y are integers and \"\n",
    "                             \"the value after colon is either 0 or 1 (1=include, 0=exclude)\")\n",
    "\n",
    "        points = args.points.split(';')\n",
    "        for point in points:\n",
    "            try:\n",
    "                coords, label = point.split(':')\n",
    "                if label not in ['0', '1']:\n",
    "                    raise ValueError(f\"Invalid label value: {label}. Must be either 0 or 1\")\n",
    "\n",
    "                try:\n",
    "                    x, y = map(int, coords.split(','))\n",
    "                except ValueError:\n",
    "                    raise ValueError(f\"Invalid coordinates: {coords}. Must be integers\")\n",
    "\n",
    "                point_type = \"include\" if label == \"1\" else \"exclude\"\n",
    "                processor.add_point(x, y, args.frame, point_type)\n",
    "            except Exception as e:\n",
    "                raise ValueError(f\"Error parsing point '{point}': {str(e)}\")\n",
    "\n",
    "    # Generate mask\n",
    "    processor.get_mask(args.checkpoint)\n",
    "\n",
    "    # Propagate to all frames\n",
    "    sample_frames = processor.propagate(\"check\", args.checkpoint)\n",
    "    video_output = processor.propagate(\"render\", args.checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65152fb3-eced-4e39-9ef7-445183243e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 existing frames, skipping extraction\n",
      "Using existing frames from: /home/fortson/ribei056/software/python/SAM2Long/frames_output_images/frames_sav_000001_copy\n",
      "First frame at: /home/fortson/ribei056/software/python/SAM2Long/frames_output_images/frames_sav_000001_copy/00000.jpg\n",
      "Added include point at (150, 500) starting at frame 6\n",
      "Loading checkpoint: tiny, (/users/5/ribei056/software/python/sam2/checkpoints/sam2.1_hiera_tiny.pt, //users/5/ribei056/software/python/SAM2Long/sam2/configs/sam2.1/sam2.1_hiera_t.yaml)\n",
      "Model loaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "frame loading (JPEG): 100%|██████████| 5/5 [00:00<00:00, 21.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial mask generated and saved to /home/fortson/ribei056/software/python/SAM2Long/frames_output_images/output_first_frame.jpg\n",
      "Cleaned up 4 temporary files from /home/fortson/ribei056/software/python/SAM2Long/frames_output_images/frames_output_images\n",
      "Starting mask propagation across 5 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 4/4 [00:42<00:00, 10.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask propagation completed in 42.37 seconds\n",
      "Processing mask results...\n",
      "Rendering 1 frames with masks...\n",
      "Processing batch 1/1...\n",
      "Batch completed in 0.00 seconds (0.00 frames/second)\n",
      "Total processing time: 42.37 seconds\n",
      "Generated 0 sample frames with masks\n",
      "Cleaned up 0 temporary files from /home/fortson/ribei056/software/python/SAM2Long/frames_output_images/frames_output_images\n",
      "Starting mask propagation across 5 frames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "propagate in video: 100%|██████████| 4/4 [00:28<00:00,  7.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask propagation completed in 28.95 seconds\n",
      "Processing mask results...\n",
      "Rendering 5 frames with masks...\n",
      "Processing batch 1/1...\n",
      "Batch completed in 0.53 seconds (7.59 frames/second)\n",
      "Total processing time: 29.47 seconds\n",
      "Creating output video...\n",
      "Running command: ffmpeg -framerate 4.0 -i /home/fortson/ribei056/software/python/SAM2Long/frames_output_images/frames_output_images/frame_%d.jpg -c:v mjpeg /home/fortson/ribei056/software/python/SAM2Long/frames_output_images/output_video.mp4\n",
      "Video creation completed in 0.14 seconds\n",
      "Generated output video: /home/fortson/ribei056/software/python/SAM2Long/frames_output_images/output_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 11.2.0 (Anaconda gcc)\n",
      "  configuration: --prefix=/users/5/ribei056/.conda/envs/samlong --cc=/croot/ffmpeg_1743153284778/_build_env/bin/x86_64-conda-linux-gnu-cc --ar=/croot/ffmpeg_1743153284778/_build_env/bin/x86_64-conda-linux-gnu-ar --nm=/croot/ffmpeg_1743153284778/_build_env/bin/x86_64-conda-linux-gnu-nm --ranlib=/croot/ffmpeg_1743153284778/_build_env/bin/x86_64-conda-linux-gnu-ranlib --strip=/croot/ffmpeg_1743153284778/_build_env/bin/x86_64-conda-linux-gnu-strip --disable-doc --enable-swresample --enable-swscale --enable-openssl --enable-libxml2 --enable-libtheora --enable-demuxer=dash --enable-postproc --enable-hardcoded-tables --enable-libfreetype --enable-libharfbuzz --enable-libfontconfig --enable-libdav1d --enable-zlib --enable-libaom --enable-pic --enable-shared --disable-static --disable-gpl --enable-version3 --disable-sdl2 --enable-libopenh264 --enable-libopus --enable-libmp3lame --enable-libopenjpeg --enable-libvorbis --enable-pthreads --enable-libtesseract --enable-libvpx\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "Input #0, image2, from '/home/fortson/ribei056/software/python/SAM2Long/frames_output_images/frames_output_images/frame_%d.jpg':\n",
      "  Duration: 00:00:01.00, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: mjpeg (Baseline), yuvj420p(pc, bt470bg/unknown/unknown), 600x400 [SAR 100:100 DAR 3:2], 4 fps, 4 tbr, 4 tbn\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mjpeg (native) -> mjpeg (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, mp4, to '/home/fortson/ribei056/software/python/SAM2Long/frames_output_images/output_video.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf60.16.100\n",
      "  Stream #0:0: Video: mjpeg (mp4v / 0x7634706D), yuvj420p(pc, bt470bg/unknown/unknown, progressive), 600x400 [SAR 100:100 DAR 3:2], q=2-31, 200 kb/s, 4 fps, 16384 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc60.31.102 mjpeg\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/200000 buffer size: 0 vbv_delay: N/A\n",
      "[out#0/mp4 @ 0xab7640] video:69kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 1.197630%\n",
      "frame=    4 fps=0.0 q=1.6 Lsize=      70kB time=00:00:00.75 bitrate= 767.0kbits/s speed=27.1x    \n"
     ]
    }
   ],
   "source": [
    "# Initialize the processor\n",
    "processor = SAM2LongProcessor()\n",
    "##%%\n",
    "# Process a video file\n",
    "first_frame = processor.preprocess_video(\"sav_dataset/example/sav_000001_copy.mp4\",outdir=\"/home/fortson/ribei056/software/python/SAM2Long/frames_output_images\" )\n",
    "##%%\n",
    "start_frame_idx =3\n",
    "# Add points to track (include points)\n",
    "processor.add_point(150, 500,start_frame_idx, \"include\")  # x, y coordinates\n",
    "# processor.add_point(250, 300, \"include\")\n",
    "\n",
    "# Add exclude points if needed\n",
    "# processor.add_point(150, 150, \"exclude\")\n",
    "\n",
    "# Generate the initial mask (using the tiny model for speed)\n",
    "mask_output = processor.get_mask(checkpoint=\"tiny\")\n",
    "\n",
    "# Check the results at intervals\n",
    "sample_frames = processor.propagate(vis_frame_type=\"check\", checkpoint=\"tiny\")\n",
    "##%%\n",
    "# Generate the final output video with masks\n",
    "video_output = processor.propagate(vis_frame_type=\"render\", checkpoint=\"tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2795b2-288e-49ea-b788-b3de633c041c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SAM)",
   "language": "python",
   "name": "samlong"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
